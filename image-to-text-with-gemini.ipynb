{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12385436,"sourceType":"datasetVersion","datasetId":7809747},{"sourceId":12385619,"sourceType":"datasetVersion","datasetId":7809841},{"sourceId":12397628,"sourceType":"datasetVersion","datasetId":7817992},{"sourceId":12406620,"sourceType":"datasetVersion","datasetId":7824068}],"dockerImageVersionId":31042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Initiate gemini","metadata":{}},{"cell_type":"code","source":"from google import genai\n\nclient = genai.Client(api_key=\"AIzaSyArx0uS3hP_gW69TmLaVEX_stnY1AZ8e14\")\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\", contents=\"president of bangladesh\"\n)\n\nprint(response.text)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:27:05.981063Z","iopub.execute_input":"2025-07-07T08:27:05.981323Z","iopub.status.idle":"2025-07-07T08:27:09.451025Z","shell.execute_reply.started":"2025-07-07T08:27:05.981298Z","shell.execute_reply":"2025-07-07T08:27:09.450214Z"}},"outputs":[{"name":"stdout","text":"The current President of Bangladesh is **Mohammed Shahabuddin**.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Extract file ","metadata":{}},{"cell_type":"code","source":"# Extract file \n\nimport os\n\n# Path to the folder where images are extracted\nmemes_folder_path = '/kaggle/input/memes-rar/Memes'\n\n# List all files in the folder\nfile_list = os.listdir(memes_folder_path)\n\n# Display the first few files to confirm\nprint(f\"First few files in the folder {memes_folder_path}:\")\nprint(file_list[:1000])  # Show the first 10 files\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\n# Path to the folder where images are extracted (from Kaggle's read-only input directory)\nmemes_folder_path = '/kaggle/input/renamed-file-number-wise'\n\n# Path to the writable directory (where we will move the files)\nwritable_folder_path = '/kaggle/working/memes_renamed'  # Create a new folder in /kaggle/working\n\n# Create the writable folder if it doesn't exist\nos.makedirs(writable_folder_path, exist_ok=True)\n\n# List all files in the folder\nfile_list = os.listdir(memes_folder_path)\n\n# Sort the files to maintain the correct order (optional, if you want to sort them)\nfile_list.sort()\n\n# Step 1: Copy files to the writable folder and rename them to 1.jpg, 2.jpg, ...\nfor idx, file_name in enumerate(file_list, start=1):\n    # Get the file extension (e.g., .jpg, .png, etc.)\n    file_extension = os.path.splitext(file_name)[1]  # Get file extension (.jpg, .png, etc.)\n    \n    # Create the new file name (sequential number without any extra text)\n    new_name = f\"{idx}{file_extension}\"\n    \n    # Get the full paths for old and new files\n    old_file_path = os.path.join(memes_folder_path, file_name)\n    new_file_path = os.path.join(writable_folder_path, new_name)\n    \n    # Copy the file from the read-only folder to the writable folder\n    shutil.copy(old_file_path, new_file_path)\n    \n    # Print the old and new names for reference\n    print(f\"Copied and renamed: {file_name} -> {new_name}\")\n\n# Now the files are renamed as 1.jpg, 2.jpg, ..., and copied to /kaggle/working/memes_renamed\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Check file","metadata":{}},{"cell_type":"code","source":"writable_folder_path ='/kaggle/working/memes_renamed'\nfile_list = os.listdir(writable_folder_path)\nprint(len(file_list))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google import genai\n\nclient = genai.Client(api_key=\"AIzaSyCm5KFaUR7EiHpZzlioB-E4-56I4klASvA\")\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\", contents=\"who ami i\"\n)\n\nprint(response.text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check which model is avaialbe ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google import genai\n\n# Initialize the client with your API key\nclient = genai.Client(api_key=\"AIzaSyCm5KFaUR7EiHpZzlioB-E4-56I4klASvA\")\n\n# List of model names to test\nmodels_to_test = [\n    \"gemini-1.5-flash\", \n    \"gemini-1.5-pro\", \n    \"gemini-2.5-flash\", \n    \"gemini-2.5-pro\", \n    \"gemini-2.5-flash-lite\", \n    \"gemini-2.5-flash-preview\"\n    # Add more models here to test as per your subscription\n]\n\n# Test each model and print if it's available\nfor model_name in models_to_test:\n    try:\n        # Try to generate content with the model\n        response = client.models.generate_content(\n            model=model_name,\n            contents=\"Who are you?\"\n        )\n        print(f\"Model '{model_name}' is available! Response:\")\n        print(response.text)  # Print the response from the model\n    except Exception as e:\n        print(f\"Error: Model '{model_name}' is not available. Error: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save it","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport pandas as pd\n\n# Path to the folder where renamed images are stored\nimage_folder_path = '/kaggle/working/memes_renamed'\n\n# Define the output CSV file path\noutput_csv_path = '/kaggle/working/renamed_memes_text.csv'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Define the prompt to send to the Gemini Pro model\n\nprompt = \"\"\"\nPlease extract all the text from the image. The first task is to **extract everything** from the image, including all dialogue parts. If the image already contains a **caption** or **character labels** (like **[Me]**, **[He]**, etc.), **do not reassign or override** the existing labels. If the labels are already present in the text, **keep them as they are**.\n\n**Important:** **Do not translate the text.** If the text is in **Bengali** or any other language, **keep it in the original language**. Do not translate the text into English or any other language.\n\nAfter the text is extracted:\n1. **Categorize the dialogue** by identifying who is speaking in the image, only for parts that do not already have a character label. For example:\n   - If the text is spoken by the **father**, label it as **[father]**.\n   - If the text is spoken by the **son**, label it as **[son]**.\n   - If the dialogue is between a **boyfriend** and **girlfriend**, label the parts as **[boyfriend]** and **[girlfriend]**.\n\n2. If there are multiple characters speaking in sequence, maintain the **correct order**. For example, if **Girlfriend** speaks first and **Boyfriend** responds, output them in that order.\n3. **Do not repeat** the dialogue for any character. Each part should be uniquely attributed to one character.\n4. If the model is unable to categorize the character, label the dialogue as **[caption]** instead of any character. For example, if there is no clear character to attribute, output the text as **[caption]** followed by the extracted text.\n5. Only output the dialogue in the following format:\n   - **[character]: [text]**\n   - If the label already exists in the image, **do not modify** it.\n6. **Avoid including any social media handles** if present in the image.\n7. The model should base its categorization on **visual context** and **dialogue cues** from the image.\n\nExample:\nIf the image contains a conversation like:\n- [Me]: \"Hey, what's up?\"\n- [He]: \"Not much, you?\"\n- [Me]: \"Just chilling!\"\n- [He]: \"Cool!\"\n\nThe output should be:\n[Me]: Hey, what's up?\n[He]: Not much, you?\n[Me]: Just chilling!\n[He]: Cool!\n\nIf the model cannot categorize the character, it should label the dialogue as **[caption]**:\n[caption]: \"This is an ambiguous part of the text with no clear character.\"\n\nMake sure the output only contains **dialogue** in the specified format, with **no preamble or explanation**.\n\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate image to text","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nfrom PIL import Image\n\n# Step 4: Function to process the first 700 images and generate text with a sleep mechanism\ndef process_images_from_folder(folder_path, model, prompt, max_images, sleep_time=20):\n    # List all image files in the folder\n    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    \n    # Sort the files numerically by the number in the filename\n    image_files.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))  # Sort by numeric part of the filename\n    \n    # Take only the first `max_images` images (50 images per session for Free Tier)\n    images_to_process = image_files[:max_images]  # This ensures we only process the first `max_images` files\n    \n    # Initialize a list to store the results for the CSV\n    results = []\n    \n    # Loop through the images in sorted numerical order\n    for idx, image_name in enumerate(images_to_process, start=1):\n        image_path = os.path.join(folder_path, image_name)\n        img = Image.open(image_path)\n\n        time.sleep(5)\n        \n        # Generate text using the model \n        response = client.models.generate_content(\n            model=\"gemini-2.5-flash\", \n            contents=[img, prompt]\n        )\n        \n        time.sleep(sleep_time)\n        \n        # Add the result to the list (image_name, generated comment, and placeholder \"Hate\")\n        results.append([image_name, response.text, \"\"])  # \"Hate\" is empty for now\n        \n        # Print status every 10 images processed for progress monitoring\n        if idx % 10 == 0:\n            print(f\"Processed {idx}/{max_images} images.\")\n        \n        # Sleep to prevent hitting the rate limit (adjust time as needed)\n        time.sleep(sleep_time)  # Wait to stay within the RPM and RPD limits\n        \n    return results\n\n# Step 5: Example of saving results to a CSV\ndef save_results_to_csv(results, output_csv_path):\n    import pandas as pd\n    df = pd.DataFrame(results, columns=[\"image_name\", \"comment\", \"Hate\"])\n    df.to_csv(output_csv_path, index=False)\n    print(f\"Results saved to {output_csv_path}\")\n\n# Example usage (processing 50 images at a time with sleep):\noutput_csv_path = \"/kaggle/working/renamed_memes_text.csv\"\n\nresults = process_images_from_folder('/kaggle/working/memes_renamed', client, prompt, max_images=50)\n\nsave_results_to_csv(results, output_csv_path)\n\n\n\n\n# Step 6: Process the first 10 images and save the results in a CSV\n\n\nresults = process_images_from_folder('/kaggle/working/memes_renamed', client, prompt)\n\nsave_results_to_csv(results, output_csv_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nfrom PIL import Image\n\n# Step 4: Function to process the first 700 images and generate text with a sleep mechanism\ndef process_images_from_folder(folder_path, model, prompt, max_images, sleep_time=5):\n    # Check if sorted image list already exists (saved previously)\n    sorted_images_file = '/kaggle/working/sorted_image_list.txt'\n    \n    # If sorted list doesn't exist, sort the images and save the list\n    if not os.path.exists(sorted_images_file):\n        # List all image files in the folder\n        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n        \n        # Sort the files numerically by the number in the filename\n        image_files.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))  # Sort by numeric part of the filename\n        \n        # Save the sorted image list to a file\n        with open(sorted_images_file, 'w') as f:\n            for image in image_files:\n                f.write(image + '\\n')  # Write each image name in a new line\n        \n        print(f\"Sorted image list saved to {sorted_images_file}.\")\n    else:\n        # If sorted list file exists, read the image list from the file\n        with open(sorted_images_file, 'r') as f:\n            image_files = [line.strip() for line in f.readlines()]\n        \n        print(f\"Loaded sorted image list from {sorted_images_file}.\")\n\n    \n    \n    # Take only the first `max_images` images\n    images_to_process = image_files[51:60]\n    \n    \n    \n    # Initialize a list to store the results for the CSV\n    results = []\n    \n    # Loop through the images in sorted numerical order\n    for idx, image_name in enumerate(images_to_process, start=1):\n        image_path = os.path.join(folder_path, image_name)\n        img = Image.open(image_path)\n\n        time.sleep(5)\n        \n        # Generate text using the model \n        response = client.models.generate_content(\n            model=\"gemini-2.5-flash\", \n            contents=[img, prompt]\n        )\n        \n        time.sleep(sleep_time)\n        \n        # Add the result to the list (image_name, generated comment, and placeholder \"Hate\")\n        results.append([image_name, response.text, \"\"])  # \"Hate\" is empty for now\n        \n        # Print status every 10 images processed for progress monitoring\n        if idx % 10 == 0:\n            print(f\"Processed {idx}/{max_images} images.\")\n        \n        # Sleep to prevent hitting the rate limit (adjust time as needed)\n        time.sleep(sleep_time)  # Wait to stay within the RPM and RPD limits\n        \n    return results\n\n# Step 5: Example of saving results to a CSV\ndef save_results_to_csv(results, output_csv_path):\n    import pandas as pd\n    df = pd.DataFrame(results, columns=[\"image_name\", \"comment\", \"Hate\"])\n    df.to_csv(output_csv_path, index=False)\n    print(f\"Results saved to {output_csv_path}\")\n\n# Example usage (processing 50 images at a time with sleep):\noutput_csv_path = \"/kaggle/working/renamed_memes_text.csv\"\n\nresults = process_images_from_folder('/kaggle/working/memes_renamed', client, prompt, max_images=50)\n\nsave_results_to_csv(results, output_csv_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nfrom PIL import Image\nimport pandas as pd\n\n# Step 4: Function to process the first 700 images and generate text with a sleep mechanism\ndef process_images_from_folder(folder_path, model, prompt, start_image, end_image, sleep_time=5):\n    # Check if sorted image list already exists (saved previously)\n    sorted_images_file = '/kaggle/working/sorted_image_list.txt'\n    \n    # If sorted list doesn't exist, sort the images and save the list\n    if not os.path.exists(sorted_images_file):\n        # List all image files in the folder\n        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n        \n        # Sort the files numerically by the number in the filename\n        image_files.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))  # Sort by numeric part of the filename\n        \n        # Save the sorted image list to a file\n        with open(sorted_images_file, 'w') as f:\n            for image in image_files:\n                f.write(image + '\\n')  # Write each image name in a new line\n        \n        print(f\"Sorted image list saved to {sorted_images_file}.\")\n    else:\n        # If sorted list file exists, read the image list from the file\n        with open(sorted_images_file, 'r') as f:\n            image_files = [line.strip() for line in f.readlines()]\n        \n        print(f\"Loaded sorted image list from {sorted_images_file}.\")\n\n    # Select the images based on the given range\n    images_to_process = image_files[start_image-1:end_image]  # 0-based indexing adjustment\n    \n    # Initialize a list to store the results for the CSV\n    results = []\n    \n    # Loop through the images in sorted numerical order\n    for idx, image_name in enumerate(images_to_process, start=start_image):\n        image_path = os.path.join(folder_path, image_name)\n        img = Image.open(image_path)\n\n        time.sleep(5)\n        \n        # Generate text using the model \n        response = client.models.generate_content(\n            model=\"gemini-2.5-flash\", \n            contents=[img, prompt]\n        )\n        \n        time.sleep(sleep_time)\n        \n        # Add the result to the list (image_name, generated comment, and placeholder \"Hate\")\n        results.append([image_name, response.text, \"\"])  # \"Hate\" is empty for now\n        \n        # Print status every 10 images processed for progress monitoring\n        if idx % 1 == 0:\n            print(f\"Processed {idx}/{end_image-start_image+1} images.\")\n        \n        # Sleep to prevent hitting the rate limit (adjust time as needed)\n        time.sleep(sleep_time)  # Wait to stay within the RPM and RPD limits\n        \n    return results\n\n# Step 5: Example of saving results to a CSV with dynamic file naming\ndef save_results_to_csv(results, start_image, end_image):\n    \n    \n    # Create the dynamic file name based on the range of images being processed\n    output_csv_path = f\"/kaggle/working/{start_image}_to_{end_image}_memes_text.csv\"\n    \n    # Create a DataFrame from results and save to CSV\n    df = pd.DataFrame(results, columns=[\"image_name\", \"comment\", \"Hate\"])\n    df.to_csv(output_csv_path, index=False)\n    print(f\"Results saved to {output_csv_path}\")\n    \n    return output_csv_path\n\n# Example usage (processing images 51 to 60 with sleep):\nstart_image = 50\nend_image = 52\n\n\nresults = process_images_from_folder('/kaggle/working/memes_renamed', client, prompt, start_image, end_image)\n\n# Save results with a dynamic CSV file name\noutput_csv_path = save_results_to_csv(results, start_image, end_image)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# last ","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nfrom PIL import Image\nimport pandas as pd\n\n\n\ndef process_images_from_folder(folder_path, model, prompt, start_image, end_image, sleep_time=5):\n    # Check if sorted image list already exists (saved previously)\n    sorted_images_file = '/kaggle/working/sorted_image_list.txt'\n    \n    # If sorted list doesn't exist, sort the images and save the list\n    \n    if not os.path.exists(sorted_images_file):\n        # List all image files in the folder\n        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n        \n        # Sort the files numerically by the number in the filename\n        image_files.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))\n        \n        # Save the sorted image list to a file\n        with open(sorted_images_file, 'w') as f:\n            for image in image_files:\n                f.write(image + '\\n')\n        \n        print(f\"Sorted image list saved to {sorted_images_file}.\")\n    else:\n        # If sorted list file exists, read the image list from the file\n        with open(sorted_images_file, 'r') as f:\n            image_files = [line.strip() for line in f.readlines()]\n        \n        print(f\"Loaded sorted image list from {sorted_images_file}.\")\n\n    # Select the images based on the given range\n    images_to_process = image_files[start_image-1:end_image]  # 0-based indexing adjustment\n    \n    # Check if there's a previously processed image (to resume from)\n    processed_images_file = '/kaggle/working/processed_images.txt'\n    \n    if os.path.exists(processed_images_file):\n        # Read the last processed image index from the file\n        with open(processed_images_file, 'r') as f:\n            last_processed = int(f.read())\n        print(f\"Resuming from image {last_processed + 1}.\")\n        \n        # Calculate how many images from current batch have been processed\n        images_already_processed = max(0, last_processed - start_image + 1)\n        images_to_process = images_to_process[images_already_processed:]\n        current_index = last_processed + 1\n    else:\n        current_index = start_image\n    \n    # Initialize CSV file path\n    output_csv_path = f\"/kaggle/working/{start_image}_to_{end_image}_memes_text.csv\"\n    \n    # Create CSV header if file doesn't exist\n    if not os.path.exists(output_csv_path):\n        df_header = pd.DataFrame(columns=[\"image_name\", \"comment\", \"Hate\"])\n        df_header.to_csv(output_csv_path, index=False)\n    \n    # Loop through the remaining images and process them\n    for image_name in images_to_process:\n        image_path = os.path.join(folder_path, image_name)\n        \n        try:\n            img = Image.open(image_path)\n            \n            # Generate text using the model \n            response = client.models.generate_content(\n                model=\"gemini-2.5-pro\", \n                contents=[img, prompt]\n            )\n            \n            # Create a single result entry\n            result = [image_name, response.text, \"\"]\n            \n            # Append this single result to CSV\n            df_single = pd.DataFrame([result], columns=[\"image_name\", \"comment\", \"Hate\"])\n            df_single.to_csv(output_csv_path, mode='a', header=False, index=False)\n            \n            # Print status\n            print(f\"Processed image {current_index}: {image_name}\")\n            \n            # Update the progress by saving the last processed image index\n            with open(processed_images_file, 'w') as f:\n                f.write(str(current_index))\n            \n            current_index += 1\n            \n        except Exception as e:\n            print(f\"Error processing {image_name}: {str(e)}\")\n            continue\n        \n        # Sleep to prevent hitting the rate limit\n        time.sleep(sleep_time)\n    \n    print(f\"Processing complete. Results saved to {output_csv_path}\")\n\n\n\n# Driver Code\n\nstart_image = 237\nend_image = 300\n\n\n\n\nprocess_images_from_folder('/kaggle/working/memes_renamed', client, prompt, start_image, end_image)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nfrom PIL import Image\nimport pandas as pd\n\ndef process_images_from_folder(folder_path, model, prompt, start_image, end_image, sleep_time=5):\n    # Check if sorted image list already exists (saved previously)\n    sorted_images_file = '/kaggle/working/sorted_image_list.txt'\n    \n    # If sorted list doesn't exist, sort the images and save the list\n    if not os.path.exists(sorted_images_file):\n        # List all image files in the folder\n        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n        \n        # Sort the files numerically by the number in the filename\n        image_files.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))\n        \n        # Save the sorted image list to a file\n        with open(sorted_images_file, 'w') as f:\n            for image in image_files:\n                f.write(image + '\\n')\n        \n        print(f\"Sorted image list saved to {sorted_images_file}.\")\n    else:\n        # If sorted list file exists, read the image list from the file\n        with open(sorted_images_file, 'r') as f:\n            image_files = [line.strip() for line in f.readlines()]\n        \n        print(f\"Loaded sorted image list from {sorted_images_file}.\")\n\n    # Select the images based on the given range\n    images_to_process = image_files[start_image-1:end_image]  # 0-based indexing adjustment\n    \n    # Initialize CSV file path\n    output_csv_path = f\"/kaggle/working/{start_image}_to_{end_image}_memes_text.csv\"\n    \n    # Create CSV header if file doesn't exist\n    if not os.path.exists(output_csv_path):\n        df_header = pd.DataFrame(columns=[\"image_name\", \"comment\", \"Hate\"])\n        df_header.to_csv(output_csv_path, index=False)\n    \n    # Initialize current index\n    current_index = start_image\n    \n    # Loop through the images and process them\n    for image_name in images_to_process:\n        image_path = os.path.join(folder_path, image_name)\n        \n        try:\n            img = Image.open(image_path)\n            \n            # Generate text using the model \n            response = client.models.generate_content(\n                model=\"gemini-2.5-pro\", \n                contents=[img, prompt]\n            )\n            \n            # Create a single result entry\n            result = [image_name, response.text, \"\"]\n            \n            # Append this single result to CSV\n            df_single = pd.DataFrame([result], columns=[\"image_name\", \"comment\", \"Hate\"])\n            df_single.to_csv(output_csv_path, mode='a', header=False, index=False)\n            \n            # Print status\n            print(f\"Processed image {current_index}: {image_name}\")\n            \n            current_index += 1\n            \n        except Exception as e:\n            print(f\"Error processing {image_name}: {str(e)}\")\n            current_index += 1\n            continue\n        \n        # Sleep to prevent hitting the rate limit\n        time.sleep(sleep_time)\n    \n    print(f\"Processing complete. Results saved to {output_csv_path}\")\n\n# Driver Code\n\nstart_image = 400\nend_image = 500\n\nprocess_images_from_folder('/kaggle/working/memes_renamed', client, prompt, start_image, end_image)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final ","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nfrom PIL import Image\nimport pandas as pd\n\n\nfrom google import genai\n\n\n\nclient = genai.Client(api_key=\"AIzaSyArx0uS3hP_gW69TmLaVEX_stnY1AZ8e14\")\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\", contents=\"who are you\"\n)\n\nprint(response.text)\n\n\n# Step 3: Define the prompt to send to the Gemini Pro model\n\nprompt = \"\"\"\nPlease extract all the text from the image. The first task is to **extract everything** from the image, including all dialogue parts. If the image already contains a **caption** or **character labels** (like **[Me]**, **[He]**, etc.), **do not reassign or override** the existing labels. If the labels are already present in the text, **keep them as they are**.\n\n**Important:** **Do not translate the text.** If the text is in **Bengali** or any other language, **keep it in the original language**. Do not translate the text into English or any other language.\n\nAfter the text is extracted:\n1. **Categorize the dialogue** by identifying who is speaking in the image, only for parts that do not already have a character label. For example:\n   - If the text is spoken by the **father**, label it as **[father]**.\n   - If the text is spoken by the **son**, label it as **[son]**.\n   - If the dialogue is between a **boyfriend** and **girlfriend**, label the parts as **[boyfriend]** and **[girlfriend]**.\n\n2. If there are multiple characters speaking in sequence, maintain the **correct order**. For example, if **Girlfriend** speaks first and **Boyfriend** responds, output them in that order.\n3. **Do not repeat** the dialogue for any character. Each part should be uniquely attributed to one character.\n4. If the model is unable to categorize the character, label the dialogue as **[caption]** instead of any character. For example, if there is no clear character to attribute, output the text as **[caption]** followed by the extracted text.\n5. Only output the dialogue in the following format:\n   - **[character]: [text]**\n   - If the label already exists in the image, **do not modify** it.\n6. **Avoid including any social media handles** if present in the image.\n7. The model should base its categorization on **visual context** and **dialogue cues** from the image.\n\nExample:\nIf the image contains a conversation like:\n- [Me]: \"Hey, what's up?\"\n- [He]: \"Not much, you?\"\n- [Me]: \"Just chilling!\"\n- [He]: \"Cool!\"\n\nThe output should be:\n[Me]: Hey, what's up?\n[He]: Not much, you?\n[Me]: Just chilling!\n[He]: Cool!\n\nIf the model cannot categorize the character, it should label the dialogue as **[caption]**:\n[caption]: \"This is an ambiguous part of the text with no clear character.\"\n\nMake sure the output only contains **dialogue** in the specified format, with **no preamble or explanation**.\n\"\"\"\n\n\n\n\ndef process_images_from_folder(folder_path, client, prompt, start_image, end_image, sleep_time=5):\n    # Available Gemini models in order of preference\n    available_models = [\n        \"gemini-2.5-pro\",\n        \"gemini-2.5-flash\", \n        \"gemini-1.5-pro\",\n        \"gemini-1.5-flash\",\n        \"gemini-2.5-flash-lite\", \n        \"gemini-2.5-flash-preview\"\n    ]\n    \n    # Track which models have hit their limits\n    exhausted_models = set()\n    \n    # Check if sorted image list already exists (saved previously)\n    sorted_images_file = '/kaggle/working/sorted_image_list.txt'\n    \n    # If sorted list doesn't exist, sort the images and save the list\n    if not os.path.exists(sorted_images_file):\n        # List all image files in the folder\n        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n        \n        # Sort the files numerically by the number in the filename\n        image_files.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))\n        \n        # Save the sorted image list to a file\n        with open(sorted_images_file, 'w') as f:\n            for image in image_files:\n                f.write(image + '\\n')\n        \n        print(f\"Sorted image list saved to {sorted_images_file}.\")\n    else:\n        # If sorted list file exists, read the image list from the file\n        with open(sorted_images_file, 'r') as f:\n            image_files = [line.strip() for line in f.readlines()]\n        \n        print(f\"Loaded sorted image list from {sorted_images_file}.\")\n    \n    # Select the images based on the given range\n    images_to_process = image_files[start_image-1:end_image]  # 0-based indexing adjustment\n    \n    # Initialize CSV file path\n    output_csv_path = f\"/kaggle/working/{start_image}_to_{end_image}_memes_text.csv\"\n    \n    # Create CSV header if file doesn't exist\n    if not os.path.exists(output_csv_path):\n        df_header = pd.DataFrame(columns=[\"image_name\", \"comment\", \"Hate\"])\n        df_header.to_csv(output_csv_path, index=False)\n    \n    # Initialize current index\n    current_index = start_image\n    \n    def is_rate_limit_error(error_message):\n        \"\"\"Check if error is related to rate limiting\"\"\"\n        rate_limit_indicators = [\n            \"rate limit\", \"quota\", \"too many requests\", \"limit exceeded\",\n            \"429\", \"resource exhausted\", \"requests per minute\", \"requests per day\"\n        ]\n        error_str = str(error_message).lower()\n        return any(indicator in error_str for indicator in rate_limit_indicators)\n    \n    def try_with_model(model_name, img, prompt):\n        \"\"\"Try to generate content with a specific model\"\"\"\n        try:\n            response = client.models.generate_content(\n                model=model_name, \n                contents=[img, prompt]\n            )\n            return response.text, None\n        except Exception as e:\n            return None, str(e)\n    \n    # Loop through the images and process them\n    for image_name in images_to_process:\n        image_path = os.path.join(folder_path, image_name)\n        \n        try:\n            img = Image.open(image_path)\n            \n            # Try each available model until one works or all are exhausted\n            success = False\n            result_text = None\n            \n            for model in available_models:\n                if model in exhausted_models:\n                    continue  # Skip models that have hit their limits\n                \n                print(f\"Trying model: {model} for image {current_index}\")\n                \n                result_text, error = try_with_model(model, img, prompt)\n                \n                if result_text is not None:\n                    # Success! Break out of model loop\n                    success = True\n                    print(f\"‚úì Successfully processed with {model}\")\n                    break\n                else:\n                    # Check if this was a rate limit error\n                    if is_rate_limit_error(error):\n                        print(f\"‚ö† Rate limit hit for {model}: {error}\")\n                        exhausted_models.add(model)\n                        \n                        # Wait a bit before trying next model\n                        time.sleep(2)\n                    else:\n                        # Some other error, try next model\n                        print(f\"‚ö† Error with {model}: {error}\")\n                        time.sleep(1)\n            \n            # Check if all models are exhausted\n            if len(exhausted_models) >= len(available_models):\n                print(\"üö® All models have hit their rate limits. Stopping processing.\")\n                print(f\"Processed up to image {current_index - 1}\")\n                break\n            \n            if success:\n                # Create a single result entry\n                result = [image_name, result_text, \"\"]\n                \n                # Append this single result to CSV\n                df_single = pd.DataFrame([result], columns=[\"image_name\", \"comment\", \"Hate\"])\n                df_single.to_csv(output_csv_path, mode='a', header=False, index=False)\n                \n                # Print status\n                print(f\"‚úì Processed image {current_index}: {image_name}\")\n                \n                current_index += 1\n            else:\n                print(f\"‚ùå Failed to process image {current_index}: {image_name} - No working models available\")\n                current_index += 1\n                continue\n                \n        except Exception as e:\n            print(f\"‚ùå Error opening/processing {image_name}: {str(e)}\")\n            current_index += 1\n            continue\n        \n        # Sleep to prevent hitting the rate limit\n        time.sleep(sleep_time)\n    \n    print(f\"Processing complete. Results saved to {output_csv_path}\")\n    print(f\"Models that hit rate limits: {list(exhausted_models)}\")\n    print(f\"Models still available: {[m for m in available_models if m not in exhausted_models]}\")\n\n\n\n# Driver Code\n\nstart_image = 1374\nend_image = 1500\n\n\nprocess_images_from_folder('/kaggle/input/renamed-file-number-wise', client, prompt, start_image, end_image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:27:53.252840Z","iopub.execute_input":"2025-07-07T08:27:53.253387Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"I am a large language model, trained by Google.\nSorted image list saved to /kaggle/working/sorted_image_list.txt.\nTrying model: gemini-2.5-pro for image 1374\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1374: 1374.jpg\nTrying model: gemini-2.5-pro for image 1375\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1375: 1375.jpg\nTrying model: gemini-2.5-pro for image 1376\n‚ö† Error with gemini-2.5-pro: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\nTrying model: gemini-2.5-flash for image 1376\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1376: 1376.jpg\nTrying model: gemini-2.5-pro for image 1377\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1377: 1377.png\nTrying model: gemini-2.5-pro for image 1378\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1378: 1378.jpg\nTrying model: gemini-2.5-pro for image 1379\n‚ö† Error with gemini-2.5-pro: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\nTrying model: gemini-2.5-flash for image 1379\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1379: 1379.jpg\nTrying model: gemini-2.5-pro for image 1380\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1380: 1380.jpg\nTrying model: gemini-2.5-pro for image 1381\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1381: 1381.jpg\nTrying model: gemini-2.5-pro for image 1382\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1382: 1382.jpg\nTrying model: gemini-2.5-pro for image 1383\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1383: 1383.jpg\nTrying model: gemini-2.5-pro for image 1384\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1384: 1384.jpg\nTrying model: gemini-2.5-pro for image 1385\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1385: 1385.jpg\nTrying model: gemini-2.5-pro for image 1386\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1386: 1386.jpg\nTrying model: gemini-2.5-pro for image 1387\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1387: 1387.jpg\nTrying model: gemini-2.5-pro for image 1388\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1388: 1388.jpg\nTrying model: gemini-2.5-pro for image 1389\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1389: 1389.png\nTrying model: gemini-2.5-pro for image 1390\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1390: 1390.jpg\nTrying model: gemini-2.5-pro for image 1391\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1391: 1391.jpg\nTrying model: gemini-2.5-pro for image 1392\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1392: 1392.jpg\nTrying model: gemini-2.5-pro for image 1393\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1393: 1393.jpg\nTrying model: gemini-2.5-pro for image 1394\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1394: 1394.jpg\nTrying model: gemini-2.5-pro for image 1395\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1395: 1395.jpg\nTrying model: gemini-2.5-pro for image 1396\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1396: 1396.jpg\nTrying model: gemini-2.5-pro for image 1397\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1397: 1397.jpg\nTrying model: gemini-2.5-pro for image 1398\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1398: 1398.jpg\nTrying model: gemini-2.5-pro for image 1399\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1399: 1399.jpg\nTrying model: gemini-2.5-pro for image 1400\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1400: 1400.jpg\nTrying model: gemini-2.5-pro for image 1401\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1401: 1401.png\nTrying model: gemini-2.5-pro for image 1402\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1402: 1402.jpg\nTrying model: gemini-2.5-pro for image 1403\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1403: 1403.png\nTrying model: gemini-2.5-pro for image 1404\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1404: 1404.jpg\nTrying model: gemini-2.5-pro for image 1405\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1405: 1405.jpg\nTrying model: gemini-2.5-pro for image 1406\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1406: 1406.jpg\nTrying model: gemini-2.5-pro for image 1407\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1407: 1407.jpg\nTrying model: gemini-2.5-pro for image 1408\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1408: 1408.jpg\nTrying model: gemini-2.5-pro for image 1409\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1409: 1409.jpg\nTrying model: gemini-2.5-pro for image 1410\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1410: 1410.jpg\nTrying model: gemini-2.5-pro for image 1411\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1411: 1411.jpg\nTrying model: gemini-2.5-pro for image 1412\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1412: 1412.jpg\nTrying model: gemini-2.5-pro for image 1413\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1413: 1413.jpg\nTrying model: gemini-2.5-pro for image 1414\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1414: 1414.jpg\nTrying model: gemini-2.5-pro for image 1415\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1415: 1415.png\nTrying model: gemini-2.5-pro for image 1416\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1416: 1416.jpg\nTrying model: gemini-2.5-pro for image 1417\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1417: 1417.jpg\nTrying model: gemini-2.5-pro for image 1418\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1418: 1418.jpg\nTrying model: gemini-2.5-pro for image 1419\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1419: 1419.jpg\nTrying model: gemini-2.5-pro for image 1420\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1420: 1420.jpg\nTrying model: gemini-2.5-pro for image 1421\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1421: 1421.jpg\nTrying model: gemini-2.5-pro for image 1422\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1422: 1422.jpg\nTrying model: gemini-2.5-pro for image 1423\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1423: 1423.jpg\nTrying model: gemini-2.5-pro for image 1424\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1424: 1424.jpg\nTrying model: gemini-2.5-pro for image 1425\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1425: 1425.jpg\nTrying model: gemini-2.5-pro for image 1426\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1426: 1426.jpg\nTrying model: gemini-2.5-pro for image 1427\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1427: 1427.png\nTrying model: gemini-2.5-pro for image 1428\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1428: 1428.jpg\nTrying model: gemini-2.5-pro for image 1429\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1429: 1429.jpg\nTrying model: gemini-2.5-pro for image 1430\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1430: 1430.jpg\nTrying model: gemini-2.5-pro for image 1431\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1431: 1431.jpg\nTrying model: gemini-2.5-pro for image 1432\n‚ö† Error with gemini-2.5-pro: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\nTrying model: gemini-2.5-flash for image 1432\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1432: 1432.jpg\nTrying model: gemini-2.5-pro for image 1433\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1433: 1433.jpg\nTrying model: gemini-2.5-pro for image 1434\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1434: 1434.jpg\nTrying model: gemini-2.5-pro for image 1435\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1435: 1435.jpg\nTrying model: gemini-2.5-pro for image 1436\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1436: 1436.jpg\nTrying model: gemini-2.5-pro for image 1437\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1437: 1437.jpg\nTrying model: gemini-2.5-pro for image 1438\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1438: 1438.jpg\nTrying model: gemini-2.5-pro for image 1439\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1439: 1439.png\nTrying model: gemini-2.5-pro for image 1440\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1440: 1440.jpg\nTrying model: gemini-2.5-pro for image 1441\n‚ö† Error with gemini-2.5-pro: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\nTrying model: gemini-2.5-flash for image 1441\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1441: 1441.jpg\nTrying model: gemini-2.5-pro for image 1442\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1442: 1442.jpg\nTrying model: gemini-2.5-pro for image 1443\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1443: 1443.jpg\nTrying model: gemini-2.5-pro for image 1444\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1444: 1444.jpg\nTrying model: gemini-2.5-pro for image 1445\n‚ö† Error with gemini-2.5-pro: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\nTrying model: gemini-2.5-flash for image 1445\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1445: 1445.jpg\nTrying model: gemini-2.5-pro for image 1446\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1446: 1446.jpg\nTrying model: gemini-2.5-pro for image 1447\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1447: 1447.jpg\nTrying model: gemini-2.5-pro for image 1448\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1448: 1448.jpg\nTrying model: gemini-2.5-pro for image 1449\n‚ö† Error with gemini-2.5-pro: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\nTrying model: gemini-2.5-flash for image 1449\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1449: 1449.jpg\nTrying model: gemini-2.5-pro for image 1450\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1450: 1450.jpg\nTrying model: gemini-2.5-pro for image 1451\n‚ö† Error with gemini-2.5-pro: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\nTrying model: gemini-2.5-flash for image 1451\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1451: 1451.png\nTrying model: gemini-2.5-pro for image 1452\n‚ö† Error with gemini-2.5-pro: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\nTrying model: gemini-2.5-flash for image 1452\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1452: 1452.jpg\nTrying model: gemini-2.5-pro for image 1453\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1453: 1453.jpg\nTrying model: gemini-2.5-pro for image 1454\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1454: 1454.jpg\nTrying model: gemini-2.5-pro for image 1455\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1455: 1455.jpg\nTrying model: gemini-2.5-pro for image 1456\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1456: 1456.jpg\nTrying model: gemini-2.5-pro for image 1457\n‚ö† Error with gemini-2.5-pro: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\nTrying model: gemini-2.5-flash for image 1457\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1457: 1457.jpg\nTrying model: gemini-2.5-pro for image 1458\n‚ö† Error with gemini-2.5-pro: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\nTrying model: gemini-2.5-flash for image 1458\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1458: 1458.jpg\nTrying model: gemini-2.5-pro for image 1459\n‚ö† Error with gemini-2.5-pro: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\nTrying model: gemini-2.5-flash for image 1459\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1459: 1459.jpg\nTrying model: gemini-2.5-pro for image 1460\n‚ö† Error with gemini-2.5-pro: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\nTrying model: gemini-2.5-flash for image 1460\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1460: 1460.jpg\nTrying model: gemini-2.5-pro for image 1461\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1461: 1461.jpg\nTrying model: gemini-2.5-pro for image 1462\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1462: 1462.jpg\nTrying model: gemini-2.5-pro for image 1463\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1463: 1463.png\nTrying model: gemini-2.5-pro for image 1464\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1464: 1464.jpg\nTrying model: gemini-2.5-pro for image 1465\n‚ö† Error with gemini-2.5-pro: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\nTrying model: gemini-2.5-flash for image 1465\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1465: 1465.jpg\nTrying model: gemini-2.5-pro for image 1466\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1466: 1466.jpg\nTrying model: gemini-2.5-pro for image 1467\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1467: 1467.jpg\nTrying model: gemini-2.5-pro for image 1468\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1468: 1468.jpg\nTrying model: gemini-2.5-pro for image 1469\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1469: 1469.jpg\nTrying model: gemini-2.5-pro for image 1470\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1470: 1470.jpg\nTrying model: gemini-2.5-pro for image 1471\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1471: 1471.jpg\nTrying model: gemini-2.5-pro for image 1472\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1472: 1472.jpg\nTrying model: gemini-2.5-pro for image 1473\n‚ö† Error with gemini-2.5-pro: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\nTrying model: gemini-2.5-flash for image 1473\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1473: 1473.jpg\nTrying model: gemini-2.5-pro for image 1474\n‚úì Successfully processed with gemini-2.5-pro\n‚úì Processed image 1474: 1474.jpg\nTrying model: gemini-2.5-pro for image 1475\n‚ö† Rate limit hit for gemini-2.5-pro: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}, 'quotaValue': '100'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '12s'}]}}\nTrying model: gemini-2.5-flash for image 1475\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1475: 1475.png\nTrying model: gemini-2.5-flash for image 1476\n‚úì Successfully processed with gemini-2.5-flash\n‚úì Processed image 1476: 1476.jpg\nTrying model: gemini-2.5-flash for image 1477\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Read the CSV file\ndf = pd.read_csv('/kaggle/working/337_to_400_memes_text.csv')\n\nprint(df)\n\nprint(\"printing specific value----------\")\n\nvalue = 40\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"value = 9\nprint(df.iloc[value]['image_name'],df.iloc[value]['comment'] ) # Access the first row (position 0))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Merge csv files","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Function to merge all CSV files from a folder into one CSV file\ndef merge_csv_files_from_folder(input_folder, output_file):\n    # List all CSV files in the folder (ensure they are sorted by name or any specific order)\n    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n    csv_files.sort()  # Optional: Sort the files if you want them in a specific order (e.g., numerically)\n    \n    # Initialize an empty DataFrame to hold the merged data\n    merged_df = pd.DataFrame()\n    \n    # Loop through the files and append them to the merged DataFrame\n    for file in csv_files:\n        file_path = os.path.join(input_folder, file)\n        \n        # Read each CSV file into a DataFrame\n        df = pd.read_csv(file_path)\n        \n        # Append the current file's data to the merged DataFrame\n        merged_df = pd.concat([merged_df, df], ignore_index=True)\n    \n    # Save the merged DataFrame to a single CSV file\n    merged_df.to_csv(output_file, index=False)\n    print(f\"Merged CSV saved to {output_file}\")\n\n# Example usage:\n# Folder containing the CSV files\ninput_folder = '/kaggle/input/batch-wise-all-data'  # Path to the folder containing the CSV files\n\n# Output file path for the merged CSV\noutput_file = '/kaggle/working/merged_output.csv'  # Path where the merged CSV will be saved\n\n# Merge the CSV files\nmerge_csv_files_from_folder(input_folder, output_file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T12:51:52.631359Z","iopub.execute_input":"2025-07-07T12:51:52.631895Z","iopub.status.idle":"2025-07-07T12:51:52.818269Z","shell.execute_reply.started":"2025-07-07T12:51:52.631870Z","shell.execute_reply":"2025-07-07T12:51:52.817672Z"}},"outputs":[{"name":"stdout","text":"Merged CSV saved to /kaggle/working/merged_output.csv\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\n\n# Load the datasets\nfile_1 = '/kaggle/input/final-datasets/a 1st image dataset level.xlsx'  # Update with the correct path\nfile_2 = '/kaggle/input/final-datasets/a 2nd image dataset level.xlsx'  # Update with the correct path\nfile_3 = '/kaggle/input/final-datasets/hate_speech_dataset.csv'  # Update with the correct path\n\n# Reading the datasets (Excel files for the first two)\ndf1 = pd.read_excel(file_1)\ndf2 = pd.read_excel(file_2)\n\n# Reading the third file (CSV)\ndf3 = pd.read_csv(file_3)\n\n# Extracting relevant columns from each dataset\ndf1 = df1[['comment', 'Hate']]\ndf2 = df2[['comment', 'Hate']]\ndf3 = df3[['Comment', 'Hate']]\n\n# Renaming 'Comment' to 'comment' for consistency in df3\ndf3 = df3.rename(columns={'Comment': 'comment'})\n\n# Combining the datasets\ncombined_df = pd.concat([df1, df2, df3], ignore_index=True)\n\n# Saving the combined dataset to a CSV file\n\noutput_path = '/kaggle/working/combined_final_hate_speech.csv'  # Update with the desired output path\ncombined_df.to_csv(output_path, index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T07:03:04.159469Z","iopub.execute_input":"2025-07-08T07:03:04.160002Z","iopub.status.idle":"2025-07-08T07:03:05.092022Z","shell.execute_reply.started":"2025-07-08T07:03:04.159980Z","shell.execute_reply":"2025-07-08T07:03:05.091271Z"}},"outputs":[],"execution_count":2}]}